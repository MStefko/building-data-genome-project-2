{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load the datasets\n",
    "def load_data(time_series_folder, metadata_file, skip=1):\n",
    "    # Load metadata    \n",
    "    columns_to_use = [\n",
    "        \"building_id\",\n",
    "        #\"site_id\",\n",
    "        \"sqm\",\n",
    "        \"lat\",\n",
    "        \"lng\",\n",
    "        #\"timezone\",\n",
    "        #\"industry\",\n",
    "        \"subindustry\",\n",
    "        \"heatingtype\",\n",
    "        #\"primaryspaceusage\"\n",
    "        #\"yearbuilt\",\n",
    "        #\"date_opened\",\n",
    "        #\"numberoffloors\",\n",
    "    ]\n",
    "    meta = pd.read_csv(metadata_file,\n",
    "                usecols=columns_to_use\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    #meta[\"industry\"].fillna(\"None\", inplace=True)\n",
    "    meta[\"subindustry\"].fillna(\"None\", inplace=True)\n",
    "    meta[\"heatingtype\"].fillna(\"None\", inplace=True)\n",
    "    \n",
    "    meta.dropna(inplace=True)\n",
    "    \n",
    "    # Load time series data\n",
    "    all_files = glob.glob(os.path.join(time_series_folder, \"*.csv\"))\n",
    "    df_list = []\n",
    "    for file in all_files[::skip]:\n",
    "        building_id = os.path.basename(file).split('.')[0]\n",
    "        \n",
    "        # check if building_id is in metadata\n",
    "        if building_id not in meta[\"building_id\"].values:\n",
    "            print(f\"Building ID {building_id} not found in metadata. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(file, parse_dates=True, index_col='timestamp')\n",
    "        df['building_id'] = os.path.basename(file).split('.')[0]\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day'] = df.index.day\n",
    "        df['month'] = df.index.month\n",
    "        df['year'] = df.index.year\n",
    "        df_list.append(df)\n",
    "    time_series_data = pd.concat(df_list)\n",
    "    \n",
    "    time_series_data.head()\n",
    "    \n",
    "    return time_series_data, meta\n",
    "\n",
    "# Merge datasets\n",
    "def merge_data(time_series_data, metadata):\n",
    "    merged_data = pd.merge(time_series_data, metadata, how='left', on='building_id')\n",
    "    return merged_data\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    # Handle missing values\n",
    "    print(\"Filling missing values\")\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.info()\n",
    "    df.head()\n",
    "    # Feature engineering\n",
    "\n",
    "    \n",
    "    # Convert to supervised learning problem\n",
    "    print(\"Creating dataset...\")\n",
    "    def create_dataset(data, target_col, time_step=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            a = data[i:(i + time_step)]\n",
    "            X.append(a)\n",
    "            Y.append(data[i + time_step][target_col])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    time_step = 10\n",
    "    feature_cols = ['hour', 'day', 'month', 'year', 'building_id', \n",
    "                    'airTemperature', \n",
    "                    'cloudCoverage', \n",
    "                    'dewTemperature', \n",
    "                    'precipDepth1HR', \n",
    "                    #'precipDepth6HR', \n",
    "                    'seaLvlPressure', \n",
    "                    'windDirection', \n",
    "                    'windSpeed',\n",
    "                    'sqm',\n",
    "                    'lat',\n",
    "                    'heatingtype',\n",
    "                    #'yearbuilt',\n",
    "                    'subindustry']  # Add more features as needed\n",
    "    target_col = 'electricity'\n",
    "    X, Y = create_dataset(df[feature_cols + [target_col]].values, target_col, time_step)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Build the Gradient Boosting model\n",
    "def build_model():\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, X_train, Y_train):\n",
    "    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)  # Flatten the input for the model\n",
    "    model.fit(X_train_reshaped, Y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, Y_test):\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)  # Flatten the input for the model\n",
    "    predictions = model.predict(X_test_reshaped)\n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Main function\n",
    "def main(time_series_folder, metadata_file, skip=1):\n",
    "    time_series_data, metadata = load_data(time_series_folder, metadata_file, skip=skip)\n",
    "    merged_data = merge_data(time_series_data, metadata)\n",
    "    X_train, X_test, Y_train, Y_test = preprocess_data(merged_data)\n",
    "    model = build_model()\n",
    "    train_model(model, X_train, Y_train)\n",
    "    evaluate_model(model, X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_426525/976562091.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  meta[\"subindustry\"].fillna(\"None\", inplace=True)\n",
      "/tmp/ipykernel_426525/976562091.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  meta[\"heatingtype\"].fillna(\"None\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ID Eagle_education_Paul not found in metadata. Skipping...\n",
      "Building ID Eagle_public_Missy not found in metadata. Skipping...\n",
      "Building ID Gator_assembly_Lucia not found in metadata. Skipping...\n",
      "Filling missing values\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 298239 entries, 0 to 298238\n",
      "Data columns (total 22 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   airTemperature  298239 non-null  float64\n",
      " 1   cloudCoverage   298239 non-null  float64\n",
      " 2   dewTemperature  298239 non-null  float64\n",
      " 3   precipDepth1HR  298239 non-null  float64\n",
      " 4   seaLvlPressure  298239 non-null  float64\n",
      " 5   windDirection   298239 non-null  float64\n",
      " 6   windSpeed       298239 non-null  float64\n",
      " 7   electricity     298239 non-null  float64\n",
      " 8   chilledwater    298239 non-null  float64\n",
      " 9   hotwater        298239 non-null  int64  \n",
      " 10  gas             298239 non-null  float64\n",
      " 11  water           298239 non-null  float64\n",
      " 12  building_id     298239 non-null  object \n",
      " 13  hour            298239 non-null  int32  \n",
      " 14  day             298239 non-null  int32  \n",
      " 15  month           298239 non-null  int32  \n",
      " 16  year            298239 non-null  int32  \n",
      " 17  sqm             298239 non-null  float64\n",
      " 18  lat             298239 non-null  float64\n",
      " 19  lng             298239 non-null  float64\n",
      " 20  subindustry     298239 non-null  object \n",
      " 21  heatingtype     298239 non-null  object \n",
      "dtypes: float64(14), int32(4), int64(1), object(3)\n",
      "memory usage: 45.5+ MB\n",
      "Creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_426525/976562091.py:72: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m time_series_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/buildings_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m metadata_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/metadata/metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m main(time_series_folder, metadata_file, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m79\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(time_series_folder, metadata_file, skip)\u001b[0m\n\u001b[1;32m    130\u001b[0m time_series_data, metadata \u001b[38;5;241m=\u001b[39m load_data(time_series_folder, metadata_file, skip\u001b[38;5;241m=\u001b[39mskip)\n\u001b[1;32m    131\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m merge_data(time_series_data, metadata)\n\u001b[0;32m--> 132\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m preprocess_data(merged_data)\n\u001b[1;32m    133\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[1;32m    134\u001b[0m train_model(model, X_train, Y_train)\n",
      "Cell \u001b[0;32mIn[21], line 104\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     89\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilding_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     90\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloudCoverage\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;66;03m#'yearbuilt',\u001b[39;00m\n\u001b[1;32m    102\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubindustry\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add more features as needed\u001b[39;00m\n\u001b[1;32m    103\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectricity\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 104\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m create_dataset(df[feature_cols \u001b[38;5;241m+\u001b[39m [target_col]]\u001b[38;5;241m.\u001b[39mvalues, target_col, time_step)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Split into training and testing sets\u001b[39;00m\n\u001b[1;32m    107\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 85\u001b[0m, in \u001b[0;36mpreprocess_data.<locals>.create_dataset\u001b[0;34m(data, target_col, time_step)\u001b[0m\n\u001b[1;32m     83\u001b[0m     a \u001b[38;5;241m=\u001b[39m data[i:(i \u001b[38;5;241m+\u001b[39m time_step)]\n\u001b[1;32m     84\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[0;32m---> 85\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(data[i \u001b[38;5;241m+\u001b[39m time_step][target_col])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(Y)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "time_series_folder = '../data/buildings_cleaned'\n",
    "metadata_file = '../data/metadata/metadata.csv'\n",
    "main(time_series_folder, metadata_file, skip=79)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
